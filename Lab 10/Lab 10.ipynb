{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab material is largely self-contained. We assume that every student has already taken STAT7008 or knows some basic operations of Python. Noet that you may use Anaconda to run the .ipynb file. For the installation of Anaconda, please see https://conda.io/docs/user-guide/install/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 10, you will learn how to:\n",
    "\n",
    "a. do spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries for this Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. numpy, for data array. \n",
    "\n",
    "b. sklearn, for preprocessing and modelling.\n",
    "\n",
    "c. os, for the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/renjielu/PycharmProjects/DM8017/DM_Lab10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import email.parser \n",
    "import os, sys, stat\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression as lr\n",
    "import shutil\n",
    "wd = os.getcwd() # Set your working directory. \n",
    "print wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used in Lab 10 is one of the datasets for the data mining competition associated with ICONIP 2010, see http://csmining.org/index.php/spam-email-datasets-.html. We only use the traning data since no label provided in the test data. In the training set, there are 4327 messages, containing 2949 non-spam messages (HAM) and 1378 spam messagees (SPAM). \n",
    "\n",
    "The following part is to extract all original files to the list varialbe all_emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4327\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "# FileName: Subsampling.py provided by the official website. Version 1.0 by Tao Ban, 2010.5.26\n",
    "# This function extract all the contents, ie subject and first part from the .eml file \n",
    "# and store it in a new file with the same name in the dst dir. \n",
    "\n",
    "def ExtractSubPayload (filename):\n",
    "    ''' Extract the subject and payload from the .eml file.\n",
    "    '''\n",
    "    if not os.path.exists(filename): # dest path doesnot exist\n",
    "        print \"ERROR: input file does not exist:\", filename\n",
    "        os.exit(1)\n",
    "    fp = open(filename)\n",
    "    msg = email.message_from_file(fp)\n",
    "    payload = msg.get_payload()\n",
    "    if type(payload) == type(list()) :\n",
    "        payload = payload[0] # only use the first part of payload\n",
    "    sub = msg.get('subject')\n",
    "    sub = str(sub)\n",
    "    if type(payload) != type('') :\n",
    "        payload = str(payload)\n",
    "    \n",
    "    return sub + payload\n",
    "\n",
    "def ExtractBodyFromDir ( srcdir, dstdir ):\n",
    "    '''Extract the body information from all .eml files in the srcdir and \n",
    "\n",
    "    save the file to the dstdir with the same name.'''\n",
    "    if not os.path.exists(dstdir): # dest path doesnot exist\n",
    "        os.makedirs(dstdir)  \n",
    "        \n",
    "    files = os.listdir(srcdir)\n",
    "    for file in files:\n",
    "        srcpath = os.path.join(srcdir, file)\n",
    "        dstpath = os.path.join(dstdir, file)\n",
    "        src_info = os.stat(srcpath)\n",
    "        if stat.S_ISDIR(src_info.st_mode): # for subfolders, recurse\n",
    "            ExtractBodyFromDir(srcpath, dstpath)\n",
    "        else:  # copy the file\n",
    "            body = ExtractSubPayload (srcpath)\n",
    "            dstfile = open(dstpath, 'w')\n",
    "            dstfile.write(body)\n",
    "            dstfile.close()\n",
    "\n",
    "\n",
    "###################################################################\n",
    "# main function start here\n",
    "# srcdir is the directory where the .eml are stored\n",
    "\n",
    "srcdir = wd+'/TRAINING'\n",
    "if not os.path.exists(srcdir):\n",
    "    print 'The source directory %s does not exist, exit...' % (srcdir)\n",
    "    sys.exit()\n",
    "# dstdir is the directory where the content .eml are stored\n",
    "\n",
    "dstdir = wd+'/Training'\n",
    "if not os.path.exists(dstdir):\n",
    "    print 'The destination directory is newly created.'\n",
    "    os.makedirs(dstdir)\n",
    "\n",
    "###################################################################\n",
    "ExtractBodyFromDir ( srcdir, dstdir ) \n",
    "\n",
    "# put all emails into one list. \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dirs = os.listdir(dstdir)\n",
    "\n",
    "all_emails = []\n",
    "    \n",
    "for fname in dirs:\n",
    "    f = open(dstdir + '/' + fname, 'r')\n",
    "    html = f.read()\n",
    "    f.close()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    s = soup.get_text()\n",
    "    \n",
    "    all_emails.append(s)\n",
    "\n",
    "print len(all_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the content of the second email as follows. \n",
    "\n",
    "We build a class called vectorizer by \"CountVectorizer\" in Sklearn. It converts a collection of text documents to the so-called document-term frequency matrix (DTM). Note that this is the transpose of the term-document frequency matrix defined in the lecture note. The binary DTM can be obtained by setting the argument \"binary\" in CountVectorizer as True. Its default value is False.\n",
    "\n",
    "Due to the memory limit in the laptop, we only consider the top 10000 features ordered by term frequency across the corpus (created by CountVectorizer). The stop word list we used here is 'english', which is a built-in word list in Sklearn. \n",
    "\n",
    "According to the training data, we build the corpus, and print the first eight elements of it.\n",
    "\n",
    "We transform the transformed data to numpy arrays by calling the toarray() function, see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ILUG] [OT]:Bill Gates in PeruHi guys,\n",
      "   slightly OT. This comes from The Register from last\n",
      "month, but I only came across it;\n",
      "\n",
      "http://www.theregister.co.uk/content/4/26207.html\n",
      "\n",
      "   Some/all of you may know that the government in\n",
      "Peru were going to instruct state bodies to look at\n",
      "using open source software in an attempt to save money\n",
      "over M$ licences. Well, read the above article.\n",
      "\n",
      "   Eamonn\n",
      "\n",
      "__________________________________________________\n",
      "Do You Yahoo!?\n",
      "Everything you'll ever need on one web page\n",
      "from News and Sport to Email and Music Charts\n",
      "http://uk.my.yahoo.com\n",
      "\n",
      "-- \n",
      "Irish Linux Users' Group: ilug@linux.ie\n",
      "http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\n",
      "List maintainer: listmaster@linux.ie\n",
      "\n",
      "\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "[u'hanging', u'woody', u'bringing', u'wednesday', u'errors', u'hp', u'usenet', u'designing']\n",
      "(4327, 10000)\n"
     ]
    }
   ],
   "source": [
    "# the second email in the data.\n",
    "\n",
    "print all_emails[1]\n",
    "\n",
    "# \n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=10000)\n",
    "\n",
    "vectorizer.fit(all_emails)\n",
    "\n",
    "print '-------------------------------------------------'\n",
    "print '-------------------------------------------------'\n",
    "print vectorizer.vocabulary_.keys()[:8]\n",
    "\n",
    "dt_matrix = vectorizer.transform(all_emails)\n",
    "\n",
    "print dt_matrix.shape\n",
    "DTM = dt_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on DTM above, we compute the inverse document frequency matrix by \"TfidfTransformer\". Note that \"TfidfVectorizer\" in Sklearn can give the inverse document frequency matrix directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   2.85666774  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   3.50639276  0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          3.44593479  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          4.27160359  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "IDF = TfidfTransformer(norm = None)\n",
    "\n",
    "IDFM = IDF.fit_transform(DTM)\n",
    "IDFM = IDFM.toarray()\n",
    "\n",
    "print IDFM[:2, :88]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build two logistic regression models by using DTM and IDFM, respectively. The results show that the accuracy from the latter model is slightly higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy computed from CM:  0.914028195054\n",
      "the accuracy computed from IDFM:  0.967876126647\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_labels = []\n",
    "\n",
    "f = open(wd+'/'+'SPAMTrain.label')\n",
    "tmp = f.read()\n",
    "f.close()\n",
    "\n",
    "tmp = tmp.split('\\n')\n",
    "tmp_ = [x.split(' ')[0] for x in tmp]\n",
    "\n",
    "tmp_.remove('')\n",
    "\n",
    "labels = np.array([int(x) for x in tmp_])\n",
    "\n",
    "lrg = lr()\n",
    "\n",
    "lrg.fit(DTM, labels)\n",
    "\n",
    "print 'the accuracy computed from CM: ', lrg.score(DTM, labels)\n",
    "\n",
    "\n",
    "lrg.fit(IDFM, labels)\n",
    "\n",
    "print 'the accuracy computed from IDFM: ', lrg.score(IDFM, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab material is largely self-contained. We assume that every student has already taken STAT7008 or knows some basic operations of Python. Noet that you may use Anaconda to run the .ipynb file. For the installation of Anaconda, please see https://conda.io/docs/user-guide/install/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 7, you will learn how to:\n",
    "\n",
    "a. do neural netwrok (NN). \n",
    "\n",
    "b. do convolutional neural network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries for this Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. numpy, for data array. \n",
    "\n",
    "b. tensorflow, for modelling.\n",
    "\n",
    "c. matplotlib, for plotting.\n",
    "\n",
    "d. sklearn, for data and preprocessing.\n",
    "\n",
    "e. os, for the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/renjielu/PycharmProjects/DM8017/DM_Lab8\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "wd = os.getcwd() # Set your working directory. \n",
    "print wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 8, we consider to make use of MNIST. It is a data set for handwritten digits. This data consists of handwritten digit images and labels. The lable ranges from 0 to 9, meaning 10 classes. Usually, the training size of MNIST is 60,000 and the test size is 10,000. For simplicity, we take advantage of the MNIST set provided by SKlearn. Its total size is only 1,797 (see below). If one would like to try the whole data set, please see Yann Lecun's website: http://yann.lecun.com/exdb/mnist/.  \n",
    "\n",
    "SKlearn returns the \"original\" target. So, before we train the models, we transform the target to the onehot target. \n",
    "\n",
    "We plot the first three samples. The true labels of them are 0, 1 and 2, respectively.\n",
    "\n",
    "Note that in this lab, we suppose all students have installed the package tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "(1797, 10)\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa6b9d62110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "part_MNIST = load_digits()\n",
    "\n",
    "X = part_MNIST.data / 255\n",
    "\n",
    "print X.shape\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "def onehot(data):\n",
    "    tmp_data_ = np.array(data)  # np.append(np.array(data), ['-1'])\n",
    "    integer_encoded = label_encoder.fit_transform(tmp_data_)\n",
    "    integer_encoded = integer_encoded.reshape(-1, 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "    return onehot_encoded\n",
    "\n",
    "y = onehot(part_MNIST.target)\n",
    "\n",
    "print y.shape\n",
    "print y[:5, :]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "for i in range(1, 4):\n",
    "\n",
    "    ax = fig.add_subplot(1, 3, i)\n",
    "    img = np.reshape(X[i-1, :], (8, 8))\n",
    "    plt.imshow(img, cmap='Greys_r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 1,500 data is considered to be the training data, the rest are the test data. We build a traditional neural network with two layers. Each layer has the same number of hidden units: 128. The logistic (sigmoid) function is set to be the  activation function. The output function is the softmax function, see https://en.wikipedia.org/wiki/Softmax_function for its definition. We train the model over 10 epochs. \n",
    "\n",
    "The following results show that the accuracy in test samples starts from 0.091, and ends at 0.747. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Avg accuracy 0.0909090936184\n",
      "Epoch 1. Avg accuracy 0.309764295816\n",
      "Epoch 2. Avg accuracy 0.548821568489\n",
      "Epoch 3. Avg accuracy 0.619528591633\n",
      "Epoch 4. Avg accuracy 0.666666686535\n",
      "Epoch 5. Avg accuracy 0.740740716457\n",
      "Epoch 6. Avg accuracy 0.757575750351\n",
      "Epoch 7. Avg accuracy 0.757575750351\n",
      "Epoch 8. Avg accuracy 0.754208743572\n",
      "Epoch 9. Avg accuracy 0.754208743572\n"
     ]
    }
   ],
   "source": [
    "tr_X = tf.placeholder(tf.float32, [None, X.shape[1]], name='tr_X')\n",
    "tr_y = tf.placeholder(tf.float32, [None, y.shape[1]], name='tr_y')\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([X.shape[1], 128], stddev=0.01), name='W1')\n",
    "b1 = tf.Variable(tf.random_normal([128]), name='b1')\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([128, 10], stddev=0.01), name='W2')\n",
    "b2 = tf.Variable(tf.random_normal([10]), name='b2')\n",
    "\n",
    "def model_NN():\n",
    "\n",
    "    layer1 = tf.nn.sigmoid(tf.add(tf.matmul(tr_X, W1), b1))\n",
    "    layer2 = tf.nn.sigmoid(tf.add(tf.matmul(layer1, W2), b2))\n",
    "    \n",
    "    return layer2\n",
    "\n",
    "\n",
    "model_NN_ = model_NN()\n",
    "\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.nn.softmax(model_NN_)-tr_y))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model_NN_, 1), tf.argmax(tr_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for j in range(0, 10):\n",
    "\n",
    "        for i in range(0, 1500):\n",
    "            \n",
    "            tmp_X = np.reshape(X[i, :], (1, -1))\n",
    "            tmp_y = np.reshape(y[i, :], (1, -1))\n",
    "\n",
    "            _, accuracy_val = sess.run([train_op, accuracy], feed_dict={tr_X: tmp_X, tr_y: tmp_y})\n",
    "                        \n",
    "        val_X = X[1500:, :]\n",
    "        val_y = y[1500:, :]\n",
    "        \n",
    "        accuracy_val = sess.run([accuracy], feed_dict={tr_X: val_X, tr_y: val_y})\n",
    "        \n",
    "        print('Epoch {}. Avg accuracy {}'.format(j, accuracy_val[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a CNN as follows. The whole model architecture is suggested by BinRoot (github). We only modify it slightly since the dimension of our input is much smaller than the case they studied. Two convoluntional layers and max pooling layers are considered. The fully connected layer (the last layer) has 1024 hidden units. The accuracy for each epoch on the test samples is printed. It seems that this model structure is too complicated to handle the given data. The total number of paramters is 340,106, but the total sample size is only 1,797."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Avg accuracy 0.101010099053\n",
      "Epoch 1. Avg accuracy 0.101010099053\n",
      "Epoch 2. Avg accuracy 0.101010099053\n",
      "Epoch 3. Avg accuracy 0.101010099053\n",
      "Epoch 4. Avg accuracy 0.101010099053\n",
      "Epoch 5. Avg accuracy 0.101010099053\n",
      "Epoch 6. Avg accuracy 0.101010099053\n",
      "Epoch 7. Avg accuracy 0.101010099053\n",
      "Epoch 8. Avg accuracy 0.101010099053\n",
      "Epoch 9. Avg accuracy 0.101010099053\n"
     ]
    }
   ],
   "source": [
    "# the whole model graph is suggested by BinRoot (github)  \n",
    "\n",
    "tr_X = tf.placeholder(tf.float32, [None, X.shape[1]])\n",
    "tr_y = tf.placeholder(tf.float32, [None, y.shape[1]])\n",
    "W1 = tf.Variable(tf.random_normal([4, 4, 1, 64]))\n",
    "b1 = tf.Variable(tf.random_normal([64]))\n",
    "W2 = tf.Variable(tf.random_normal([4, 4, 64, 64]))\n",
    "b2 = tf.Variable(tf.random_normal([64]))\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([2*2*64, 1024]))\n",
    "b3 = tf.Variable(tf.random_normal([1024]))\n",
    "\n",
    "W_out = tf.Variable(tf.random_normal([1024, y.shape[1]]))\n",
    "b_out = tf.Variable(tf.random_normal([y.shape[1]]))\n",
    "\n",
    "def conv_layer(x, W, b):\n",
    "    conv = tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_with_b = tf.nn.bias_add(conv, b)\n",
    "    conv_out = tf.nn.relu(conv_with_b)\n",
    "    return conv_out\n",
    "\n",
    "\n",
    "def maxpool_layer(conv, k=2):\n",
    "    return tf.nn.max_pool(conv, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def model():\n",
    "\n",
    "    tr_X_reshaped = tf.reshape(tr_X, shape=[-1, 8, 8, 1])\n",
    "    \n",
    "    conv_out1 = conv_layer(tr_X_reshaped, W1, b1)\n",
    "    maxpool_out1 = maxpool_layer(conv_out1)\n",
    "    norm1 = tf.nn.lrn(maxpool_out1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    conv_out2 = conv_layer(norm1, W2, b2)\n",
    "    norm2 = tf.nn.lrn(conv_out2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "    maxpool_out2 = maxpool_layer(norm2)\n",
    "\n",
    "    maxpool_reshaped = tf.reshape(maxpool_out2, [-1, W3.get_shape().as_list()[0]])\n",
    "    local = tf.add(tf.matmul(maxpool_reshaped, W3), b3)\n",
    "    local_out = tf.nn.relu(local)\n",
    "\n",
    "    out = tf.add(tf.matmul(local_out, W_out), b_out)\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "model_op = model()\n",
    "\n",
    "cost = tf.reduce_mean(tf.square(tf.nn.softmax(model_op)-tr_y))\n",
    "\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(model_op, 1), tf.argmax(tr_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for j in range(0, 10):\n",
    "        \n",
    "        \n",
    "        for i in range(0, 1500, 100):\n",
    "\n",
    "            \n",
    "            tmp_X = X[i:i+100, :]\n",
    "            tmp_y = y[i:i+100, :]\n",
    "\n",
    "            _, accuracy_val = sess.run([train_op, accuracy], feed_dict={tr_X: tmp_X, tr_y: tmp_y})\n",
    "            \n",
    "                        \n",
    "        val_X = X[1500:, :]\n",
    "        val_y = y[1500:, :]\n",
    "        \n",
    "        accuracy_val = sess.run([accuracy], feed_dict={tr_X: val_X, tr_y: val_y})\n",
    "        \n",
    "        print('Epoch {}. Avg accuracy {}'.format(j, accuracy_val[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

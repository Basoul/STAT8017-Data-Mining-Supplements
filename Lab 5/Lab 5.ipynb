{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Bagging and Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab material is largely self-contained. We assume that every student has already taken STAT7008 or knows some basic operations of Python. Noet that you may use Anaconda to run the .ipynb file. For the installation of Anaconda, please see https://conda.io/docs/user-guide/install/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 5, you will learn how to:\n",
    "\n",
    "a. do Bagging. \n",
    "\n",
    "b. do Boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful libraries for this Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. numpy, for data array. \n",
    "\n",
    "b. sklearn, for modelling.\n",
    "\n",
    "c. os, for the working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/renjielu/PycharmProjects/DM8017/DM_Lab5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.ensemble import BaggingClassifier as bagging\n",
    "from sklearn.ensemble import GradientBoostingClassifier as boosting\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "wd = os.getcwd() # Set your working directory. \n",
    "print wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Lab 5, we consider to make use of Iris data set to show Bagging and Boosting. The data description is printed, including sample size, summary statistics and so on. The data contains three different types of iris plant (3 classes). Each plant is characterized by 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "print iris.DESCR # the Iris description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we use a decision tree with maximum depth is 2 to be the base estimator (classifier) for Bagging. Similarly, the maximum depth in Boosting is set as 2 (default value is equal to 3). Note that if one would like to recover the AdaBoost algorithm, the exponential loss function in Boosting should be used. \n",
    "\n",
    "In order to show how the number of trees (iterations) affects the preformances of Bagging and Boosting, we consider two different number values: 10 and 100. From the results below, we conclude that (a) for Bagging, it seems no influence from the number of trees; (b) for Boosting, more trees lead to higher accuracy, which may be understood from the fact that Boosting can be approximately solved by a gradient descent method. Notice that large number of iterations may cause the overfitting problem in Boosting. \n",
    "\n",
    "A simple decision tree is considered to be the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy of tree on the given data and labels is 0.96\n",
      "When the number of trees is 10, the mean accuracy of Bagging tree on the given data and labels is 0.953\n",
      "When the number of trees is 10, the mean accuracy of Boosting tree on the given data and labels is 0.973\n",
      "When the number of trees is 100, the mean accuracy of Bagging tree on the given data and labels is 0.953\n",
      "When the number of trees is 100, the mean accuracy of Boosting tree on the given data and labels is 1.000\n"
     ]
    }
   ],
   "source": [
    "cart = DecisionTreeClassifier(max_depth=2) \n",
    "\n",
    "model_tree = cart.fit(X, y)\n",
    "print 'The mean accuracy of tree on the given data and labels is %.2f' % model_tree.score(X, y)\n",
    "\n",
    "num_trees = 10\n",
    "model_bagging = bagging(base_estimator=cart, n_estimators=num_trees)\n",
    "\n",
    "model_bagging.fit(X, y)\n",
    "\n",
    "print 'When the number of trees is %d, the mean accuracy of Bagging tree on the given data and labels is %.3f' % (num_trees, model_bagging.score(X, y))\n",
    "\n",
    "model_boosting = boosting(n_estimators=num_trees, max_depth=2)\n",
    "model_boosting.fit(X, y)\n",
    "\n",
    "print 'When the number of trees is %d, the mean accuracy of Boosting tree on the given data and labels is %.3f' % (num_trees, model_boosting.score(X, y))\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "num_trees = 100\n",
    "\n",
    "model_bagging = bagging(base_estimator=cart, n_estimators=num_trees)\n",
    "model_bagging.fit(X, y)\n",
    "\n",
    "print 'When the number of trees is %d, the mean accuracy of Bagging tree on the given data and labels is %.3f' % (num_trees, model_bagging.score(X, y))\n",
    "\n",
    "\n",
    "model_boosting = boosting(n_estimators=num_trees, max_depth=2)\n",
    "model_boosting.fit(X, y)\n",
    "print 'When the number of trees is %d, the mean accuracy of Boosting tree on the given data and labels is %.3f' % (num_trees, model_boosting.score(X, y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
